# Hidden Markov Model
## Overview
This project implements the Viterbi algorithm using a Hidden Markov Model (HMM) to predict the most likely sequence of hidden states based on a given sequence of observed states. 

The algorithm takes as input a sequence of integers representing the numbers rolled from a die, with possible values ranging from 1 to 6. In this context, the hidden states correspond to the type of dice being used, where F represents a fair die and W represents a weighted die. The output of the algorithm is the most likely sequence of hidden states (F and W) that corresponds to the observed sequence of dice rolls. 

This model is applicable in scenarios where the system being modeled follows a Markov process with hidden (unobserved) states, making it useful in various applications such as gaming simulations, speech recognition, part-of-speech tagging, and bioinformatics.
## How the algorithm works
The algorithm finds the most likely sequence of hidden states by iteratively calculating probabilities based on transition and emission probabilities. It operates as follows:

### 1. Initialization:

The algorithm begins by defining the start state and initializing the probability of reaching the start state at the first step. In this case, the start state is represented by "B" and the end state by "Z".
### 2. Dynamic Programming (Sigma Table Construction):

The Viterbi algorithm iteratively fills out a table of probabilities (sigma) using loops. This table stores the maximum log probability of any path that ends in a given state at each time step.
For each observed state (in the sequence of observations), the algorithm calculates the probability of transitioning to the next hidden state using the transition probabilities and emission probabilities. It keeps track of the most likely previous state (psi) that leads to each state, ensuring the optimal sequence is built.
### 3. Probability Calculation:

For each time step, the algorithm computes the most likely state based on the maximum probability from all possible previous states. This is done iteratively, by looping over all possible states at each time step.
### 4. Backtracking:

After constructing the sigma table, the algorithm traces back from the most likely final state (Z) to recover the most likely sequence of hidden states that led to the given sequence of observations. This is done using the psi table, which stores the most likely previous state for each state at each step.
### 5. Final Sequence:

The final sequence is obtained by backtracking through the psi table from the end state to the start state. The hidden states in this sequence represent the most likely path that the HMM followed to generate the observed sequence.
The algorithm uses log probabilities to avoid numerical underflow issues that arise from multiplying many small probabilities together. The try_log helper function ensures the algorithm works with logarithmic values, returning None if any of the input values are invalid (i.e., zero or None)

### Key terms
- Hidden states: The unknown states that we want to predict.
- Observed states: The known data used to infer the hidden states.
- Transition probabilities: The probability of moving from one hidden state to another.
- Emission probabilities: The probability of observing a particular observation given a hidden state.

## Evaluation of the model
To assess the performance of the HMM and Viterbi algorithm, I used the following metrics:
### Evaluation metrics
#### Precision
Precision measures the proportion of correctly predicted positive observations (hidden states) out of all observations that were predicted to be positive. It is calculated as:

Precision = True Positives / (True Positives + False Positives)

A high precision score means that the model makes few false-positive predictions, meaning when it predicts a certain hidden state, itâ€™s likely to be correct.

#### Recall
Recall measures the proportion of correctly predicted positive observations out of all actual positive observations. It is calculated as:

Precision = True Positives / (True Positives + False Negatives)

High recall indicates that the model successfully identifies a high proportion of actual positive states (hidden states).

#### F1 Score
The F1 score is the harmonic mean of Precision and Recall and provides a balanced metric when there is an uneven class distribution. It gives a more comprehensive evaluation by considering both false positives and false negatives. It is calculated as:

F1 = 2 * (Precision * Recall) / (Precision + Recall)

The F1 score is especially useful when you need to balance precision and recall and are interested in both avoiding false positives and false negatives.
### Cross fold validation
To ensure that the model was not overfitting or memorizing the dataset, I applied 10-fold cross-validation. This method divides the dataset into 10 equally sized subsets (folds). The model is trained on 9 of the folds and evaluated on the remaining fold, repeating this process 10 times, with each fold being used as the test set once.

The average performance across all 10 folds is used as the final evaluation metric, providing a more robust estimate of model performance on unseen data. This method helps reduce the likelihood of overfitting by ensuring that the model generalizes well to different portions of the dataset.

#### Why 10-fold cross-validation?

It provides a good balance between computational efficiency and model evaluation accuracy.
It helps minimize both bias (by training on a large portion of the data) and variance (by testing on different subsets of data).
It ensures that each data point is used for both training and validation, leading to a more reliable estimate of model performance.
## Conclusion
By using the Viterbi algorithm and Hidden Markov Models, this project successfully predicts the most likely sequence of hidden states based on a given sequence of observations. The model's performance was evaluated using Precision, Recall, and F1 Score, with additional robustness ensured through 10-fold cross-validation.

### Model Outputs

#### Evaluating HMM on a Single Training and Dev Split (Random Seed 2)

**Transition Probabilities of the HMM:**

|   |  B  |  F  |  W  |  Z  |
|---|-----|-----|-----|-----|
| B | 0.000 | 0.481 | 0.519 | 0.000 |
| F | 0.000 | 0.948 | 0.050 | 0.002 |
| W | 0.000 | 0.050 | 0.948 | 0.002 |
| Z | 0.000 | 0.000 | 0.000 | 0.000 |

**Emission Probabilities of the HMM:**

|   |  1  |  2  |  3  |  4  |  5  |  6  |  B  |  Z  |
|---|-----|-----|-----|-----|-----|-----|-----|-----|
| B | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 1.000 | 0.000 |
| F | 0.165 | 0.165 | 0.167 | 0.167 | 0.168 | 0.167 | 0.000 | 0.000 |
| W | 0.502 | 0.248 | 0.063 | 0.061 | 0.063 | 0.062 | 0.000 | 0.000 |
| Z | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 1.000 |

**Evaluation Metrics for Seed 2:**
- **Precision**: 0.8511
- **Recall**: 0.8821
- **F1 Score**: 0.8664

#### Evaluating HMM Using Cross-Validation (10 Folds)
- **Average Precision**: 0.8536
- **Average Recall**: 0.8626
- **Average F1 Score**: 0.8580
